{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "this_list = [\"pay\",\"cup\",\"ship\",\"bat\",\"these\",\"show\",\"rut\",\"down\",\"ten\",\"into\",\"then\",\"dog\",\"bat\",\"with\",\"my\",\"dig\",\"note\",\"cut\",\"mom\",\"brick\"]\n",
    "print(len(this_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/homebrew/lib/python3.11/site-packages (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Level  Words  Unique  Bonus  Unique Bonus \n",
      "     1    250     245    168           168 \n",
      "     2    250     250    168           168 \n",
      "     3    250     250    168           168 \n",
      "     4    250     250    168           168 \n",
      "     5    250     250    168           168 \n",
      "     6    250     250    168           168 \n",
      "     7    250     249    168           168 \n",
      "     8    250     250    168           168 \n",
      "     9    250     250    168           168 \n",
      "    10    250     250    168           168 \n",
      "    11    250     250    168           168 \n",
      "    12    504     502    168           168 \n",
      "    13    350     348    168           168 \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"src/data/word_lists.json\"\n",
    "\n",
    "# Read the JSON file\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Load the data into a dictionary\n",
    "word_lists = json.loads(data)\n",
    "\n",
    "# Print the number of words in each level\n",
    "def word_table(word_list):\n",
    "    from prettytable import PrettyTable\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Level\", \"Words\", \"Unique\", \"Bonus\", \"Unique Bonus\"]\n",
    "    table.border = False\n",
    "    table.align = \"r\"\n",
    "    for level in word_lists[\"levels\"]:\n",
    "        words = word_lists[\"levels\"][level][\"words\"]\n",
    "        unique_words = set(words)\n",
    "        bonus = word_lists[\"levels\"][level][\"bonus\"]\n",
    "        unique_bonus = set(bonus)\n",
    "        table.add_row([level, len(words), len(unique_words), len(bonus), len(unique_bonus)])\n",
    "\n",
    "    print(table)\n",
    "\n",
    "word_table(word_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique word lists have been saved to src/data/word_lists_unique.json\n",
      " Level  Words  Unique  Bonus  Unique Bonus \n",
      "     1    245     245    168           168 \n",
      "     2    250     250    168           168 \n",
      "     3    250     250    168           168 \n",
      "     4    250     250    168           168 \n",
      "     5    249     249    168           168 \n",
      "     6    250     250    168           168 \n",
      "     7    249     249    168           168 \n",
      "     8    250     250    168           168 \n",
      "     9    250     250    168           168 \n",
      "    10    250     250    168           168 \n",
      "    11    250     250    168           168 \n",
      "    12    502     502    168           168 \n",
      "    13    348     348    168           168 \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the file paths\n",
    "input_file_path = \"src/data/word_lists.json\"\n",
    "output_file_path = \"src/data/word_lists_unique.json\"\n",
    "\n",
    "# Read the original JSON file\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Load the data into a dictionary\n",
    "word_lists = json.loads(data)\n",
    "\n",
    "# Create a new dictionary to store the unique words\n",
    "unique_word_lists = {\n",
    "    \"levels\": {},\n",
    "    \"metadata\": word_lists[\"metadata\"]  # Copy the metadata as it is\n",
    "}\n",
    "\n",
    "# Process each level to keep only unique words, ensuring they are lowercase and stripped\n",
    "for level in word_lists[\"levels\"]:\n",
    "    words = word_lists[\"levels\"][level][\"words\"]\n",
    "    bonus_words = word_lists[\"levels\"][level][\"bonus\"]\n",
    "    \n",
    "    # Create sets with words that are lowercased and stripped of leading/trailing whitespace\n",
    "    unique_words = list(set(word.strip().lower() for word in words))\n",
    "    unique_bonus_words = list(set(bonus_word.strip().lower() for bonus_word in bonus_words))\n",
    "    \n",
    "    unique_word_lists[\"levels\"][level] = {\n",
    "        \"words\": unique_words,\n",
    "        \"bonus\": unique_bonus_words\n",
    "    }\n",
    "\n",
    "# Save the unique words dictionary to a new JSON file\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    json.dump(unique_word_lists, output_file, indent=4)\n",
    "\n",
    "print(f\"Unique word lists have been saved to {output_file_path}\")\n",
    "\n",
    "# Reuse the word_table function to verify\n",
    "def word_table(word_list):\n",
    "    from prettytable import PrettyTable\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Level\", \"Words\", \"Unique\", \"Bonus\", \"Unique Bonus\"]\n",
    "    table.border = False\n",
    "    table.align = \"r\"\n",
    "    for level in word_lists[\"levels\"]:\n",
    "        words = word_list[\"levels\"][level][\"words\"]\n",
    "        unique_words = set(words)\n",
    "        bonus = word_list[\"levels\"][level][\"bonus\"]\n",
    "        unique_bonus = set(bonus)\n",
    "        table.add_row([level, len(words), len(unique_words), len(bonus), len(unique_bonus)])\n",
    "\n",
    "    print(table)\n",
    "\n",
    "word_table(unique_word_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.3.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting httpx<0.28.0,>=0.27.0 (from ollama)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting anyio (from httpx<0.28.0,>=0.27.0->ollama)\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.6.2)\n",
      "Collecting httpcore==1.* (from httpx<0.28.0,>=0.27.0->ollama)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /opt/homebrew/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.4)\n",
      "Collecting sniffio (from httpx<0.28.0,>=0.27.0->ollama)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading ollama-0.3.2-py3-none-any.whl (10 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sniffio, h11, httpcore, anyio, httpx, ollama\n",
      "Successfully installed anyio-4.4.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 ollama-0.3.2 sniffio-1.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"openhermes\",\n",
    "    \"prompt\": \"Tell me a joke\",\n",
    "    \"stream\": False,\n",
    "    \"max_tokens\": 50\n",
    "}\n",
    "\n",
    "import requests\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "response_text = response.json()[\"response\"]\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing level 1...\n",
      "3 unique new bonus words generated for level 1 so far.\n",
      "3 unique new bonus words generated for level 1 so far.\n",
      "5 unique new bonus words generated for level 1 so far.\n",
      "8 unique new bonus words generated for level 1 so far.\n",
      "15 unique new bonus words generated for level 1 so far.\n",
      "15 unique new bonus words generated for level 1 so far.\n",
      "19 unique new bonus words generated for level 1 so far.\n",
      "19 unique new bonus words generated for level 1 so far.\n",
      "26 unique new bonus words generated for level 1 so far.\n",
      "40 unique new bonus words generated for level 1 so far.\n",
      "41 unique new bonus words generated for level 1 so far.\n",
      "43 unique new bonus words generated for level 1 so far.\n",
      "44 unique new bonus words generated for level 1 so far.\n",
      "46 unique new bonus words generated for level 1 so far.\n",
      "49 unique new bonus words generated for level 1 so far.\n",
      "49 unique new bonus words generated for level 1 so far.\n",
      "50 unique new bonus words generated for level 1 so far.\n",
      "52 unique new bonus words generated for level 1 so far.\n",
      "52 unique new bonus words generated for level 1 so far.\n",
      "55 unique new bonus words generated for level 1 so far.\n",
      "59 unique new bonus words generated for level 1 so far.\n",
      "61 unique new bonus words generated for level 1 so far.\n",
      "61 unique new bonus words generated for level 1 so far.\n",
      "64 unique new bonus words generated for level 1 so far.\n",
      "66 unique new bonus words generated for level 1 so far.\n",
      "66 unique new bonus words generated for level 1 so far.\n",
      "68 unique new bonus words generated for level 1 so far.\n",
      "68 unique new bonus words generated for level 1 so far.\n",
      "70 unique new bonus words generated for level 1 so far.\n",
      "70 unique new bonus words generated for level 1 so far.\n",
      "70 unique new bonus words generated for level 1 so far.\n",
      "71 unique new bonus words generated for level 1 so far.\n",
      "71 unique new bonus words generated for level 1 so far.\n",
      "72 unique new bonus words generated for level 1 so far.\n",
      "72 unique new bonus words generated for level 1 so far.\n",
      "74 unique new bonus words generated for level 1 so far.\n",
      "76 unique new bonus words generated for level 1 so far.\n",
      "80 unique new bonus words generated for level 1 so far.\n",
      "Adding 80 new bonus words to level 1: ['tiger', 'scape', 'gusto', 'taint', 'plum', 'pithy', 'gulf', 'wince', 'brine', 'cream', 'pork', 'stall', 'claw', 'scour', 'pelt', 'hatch', 'raze', 'gait', 'weld', 'fuss', 'hunch', 'wide', 'drake', 'silt', 'pane', 'frost', 'fret', 'cull', 'gnome', 'mirth', 'kale', 'stir', 'zeal', 'gnarl', 'lane', 'pate', 'bare', 'rash', 'stake', 'bolt', 'stoop', 'claim', 'gaze', 'tinge', 'grit', 'spare', 'hark', 'quake', 'dais', 'cane', 'vase', 'blade', 'clot', 'flask', 'kelp', 'cove', 'zone', 'shrug', 'fork', 'slush', 'plush', 'rind', 'dour', 'muck', 'gear', 'ditch', 'floss', 'brawn', 'quail', 'snipe', 'rake', 'grime', 'mossy', 'knave', 'dash', 'welt', 'vane', 'snoop', 'swift', 'swath']\n",
      "746 unique words so far.\n",
      "Processing level 2...\n",
      "3 unique new bonus words generated for level 2 so far.\n",
      "5 unique new bonus words generated for level 2 so far.\n",
      "5 unique new bonus words generated for level 2 so far.\n",
      "7 unique new bonus words generated for level 2 so far.\n",
      "8 unique new bonus words generated for level 2 so far.\n",
      "11 unique new bonus words generated for level 2 so far.\n",
      "19 unique new bonus words generated for level 2 so far.\n",
      "20 unique new bonus words generated for level 2 so far.\n",
      "25 unique new bonus words generated for level 2 so far.\n",
      "26 unique new bonus words generated for level 2 so far.\n",
      "28 unique new bonus words generated for level 2 so far.\n",
      "29 unique new bonus words generated for level 2 so far.\n",
      "31 unique new bonus words generated for level 2 so far.\n",
      "34 unique new bonus words generated for level 2 so far.\n",
      "35 unique new bonus words generated for level 2 so far.\n",
      "36 unique new bonus words generated for level 2 so far.\n",
      "38 unique new bonus words generated for level 2 so far.\n",
      "40 unique new bonus words generated for level 2 so far.\n",
      "40 unique new bonus words generated for level 2 so far.\n",
      "41 unique new bonus words generated for level 2 so far.\n",
      "42 unique new bonus words generated for level 2 so far.\n",
      "43 unique new bonus words generated for level 2 so far.\n",
      "45 unique new bonus words generated for level 2 so far.\n",
      "48 unique new bonus words generated for level 2 so far.\n",
      "49 unique new bonus words generated for level 2 so far.\n",
      "50 unique new bonus words generated for level 2 so far.\n",
      "50 unique new bonus words generated for level 2 so far.\n",
      "50 unique new bonus words generated for level 2 so far.\n",
      "50 unique new bonus words generated for level 2 so far.\n",
      "54 unique new bonus words generated for level 2 so far.\n",
      "54 unique new bonus words generated for level 2 so far.\n",
      "60 unique new bonus words generated for level 2 so far.\n",
      "60 unique new bonus words generated for level 2 so far.\n",
      "65 unique new bonus words generated for level 2 so far.\n",
      "67 unique new bonus words generated for level 2 so far.\n",
      "67 unique new bonus words generated for level 2 so far.\n",
      "68 unique new bonus words generated for level 2 so far.\n",
      "70 unique new bonus words generated for level 2 so far.\n",
      "71 unique new bonus words generated for level 2 so far.\n",
      "71 unique new bonus words generated for level 2 so far.\n",
      "72 unique new bonus words generated for level 2 so far.\n",
      "74 unique new bonus words generated for level 2 so far.\n",
      "82 unique new bonus words generated for level 2 so far.\n",
      "Adding 82 new bonus words to level 2: ['rafter', 'turtle', 'bungle', 'quota', 'pummel', 'clover', 'gales', 'nestle', 'frizz', 'jolt', 'fussed', 'bazaar', 'troll', 'drowsy', 'plies', 'rant', 'cram', 'nectar', 'oasis', 'fiddle', 'velvet', 'creaky', 'cajole', 'crusty', 'idiocy', 'coifs', 'tango', 'giddy', 'lurch', 'scurry', 'pallor', 'zigzag', 'outrun', 'whiten', 'stale', 'mishap', 'oar', 'tease', 'dabbed', 'flora', 'heed', 'fizzy', 'flaxen', 'laden', 'wanes', 'pulse', 'oval', 'sally', 'glower', 'coven', 'rancid', 'chisel', 'guffaw', 'chomp', 'sneer', 'bevy', 'wax', 'plunk', 'rasp', 'dwelt', 'groan', 'jostle', 'twinge', 'lenses', 'squint', 'wager', 'napped', 'pebble', 'tide', 'frenzy', 'grove', 'humid', 'muzzle', 'squawk', 'twitch', 'thaw', 'spritz', 'dulcet', 'wring', 'whimsy', 'charge', 'spicy']\n",
      "1423 unique words so far.\n",
      "Processing level 3...\n",
      "2 unique new bonus words generated for level 3 so far.\n",
      "5 unique new bonus words generated for level 3 so far.\n",
      "6 unique new bonus words generated for level 3 so far.\n",
      "7 unique new bonus words generated for level 3 so far.\n",
      "8 unique new bonus words generated for level 3 so far.\n",
      "9 unique new bonus words generated for level 3 so far.\n",
      "14 unique new bonus words generated for level 3 so far.\n",
      "14 unique new bonus words generated for level 3 so far.\n",
      "14 unique new bonus words generated for level 3 so far.\n",
      "14 unique new bonus words generated for level 3 so far.\n",
      "16 unique new bonus words generated for level 3 so far.\n",
      "18 unique new bonus words generated for level 3 so far.\n",
      "20 unique new bonus words generated for level 3 so far.\n",
      "24 unique new bonus words generated for level 3 so far.\n",
      "25 unique new bonus words generated for level 3 so far.\n",
      "28 unique new bonus words generated for level 3 so far.\n",
      "30 unique new bonus words generated for level 3 so far.\n",
      "31 unique new bonus words generated for level 3 so far.\n",
      "31 unique new bonus words generated for level 3 so far.\n",
      "32 unique new bonus words generated for level 3 so far.\n",
      "32 unique new bonus words generated for level 3 so far.\n",
      "32 unique new bonus words generated for level 3 so far.\n",
      "35 unique new bonus words generated for level 3 so far.\n",
      "35 unique new bonus words generated for level 3 so far.\n",
      "36 unique new bonus words generated for level 3 so far.\n",
      "40 unique new bonus words generated for level 3 so far.\n",
      "41 unique new bonus words generated for level 3 so far.\n",
      "42 unique new bonus words generated for level 3 so far.\n",
      "42 unique new bonus words generated for level 3 so far.\n",
      "43 unique new bonus words generated for level 3 so far.\n",
      "43 unique new bonus words generated for level 3 so far.\n",
      "43 unique new bonus words generated for level 3 so far.\n",
      "44 unique new bonus words generated for level 3 so far.\n",
      "45 unique new bonus words generated for level 3 so far.\n",
      "48 unique new bonus words generated for level 3 so far.\n",
      "49 unique new bonus words generated for level 3 so far.\n",
      "53 unique new bonus words generated for level 3 so far.\n",
      "57 unique new bonus words generated for level 3 so far.\n",
      "58 unique new bonus words generated for level 3 so far.\n",
      "58 unique new bonus words generated for level 3 so far.\n",
      "58 unique new bonus words generated for level 3 so far.\n",
      "58 unique new bonus words generated for level 3 so far.\n",
      "58 unique new bonus words generated for level 3 so far.\n",
      "64 unique new bonus words generated for level 3 so far.\n",
      "65 unique new bonus words generated for level 3 so far.\n",
      "66 unique new bonus words generated for level 3 so far.\n",
      "66 unique new bonus words generated for level 3 so far.\n",
      "67 unique new bonus words generated for level 3 so far.\n",
      "68 unique new bonus words generated for level 3 so far.\n",
      "70 unique new bonus words generated for level 3 so far.\n",
      "70 unique new bonus words generated for level 3 so far.\n",
      "70 unique new bonus words generated for level 3 so far.\n",
      "72 unique new bonus words generated for level 3 so far.\n",
      "73 unique new bonus words generated for level 3 so far.\n",
      "73 unique new bonus words generated for level 3 so far.\n",
      "73 unique new bonus words generated for level 3 so far.\n",
      "75 unique new bonus words generated for level 3 so far.\n",
      "79 unique new bonus words generated for level 3 so far.\n",
      "79 unique new bonus words generated for level 3 so far.\n",
      "79 unique new bonus words generated for level 3 so far.\n",
      "81 unique new bonus words generated for level 3 so far.\n",
      "Adding 81 new bonus words to level 3: ['enzyme', 'muddied', 'wilted', 'breezy', 'bevel', 'jigsaw', 'jadedly', 'hearth', 'sable', 'sleight', 'twilit', 'deride', 'dwindle', 'perplex', 'twinkle', 'bluffly', 'riddled', 'skunks', 'dapper', 'gnarled', 'humbled', 'cunning', 'scamper', 'zealous', 'plaited', 'blended', 'flocks', 'unrest', 'muggy', 'spurted', 'cloven', 'fretful', 'weary', 'yawning', 'revere', 'cuddly', 'vigor', 'tangled', 'brittle', 'cradled', 'slobber', 'zebras', 'flicker', 'sprain', 'geyser', 'zany', 'plummet', 'jazzy', 'squab', 'skimmed', 'bounced', 'clinged', 'trundle', 'razzle', 'bumped', 'piquant', 'caucus', 'ransack', 'spatter', 'creaked', 'crinkle', 'humdrum', 'blurt', 'scepter', 'gnarls', 'enigma', 'crater', 'inertia', 'cajoled', 'spryly', 'aloft', 'sniffy', 'snicker', 'kayak', 'soaring', 'indigo', 'sherbet', 'vexing', 'taffeta', 'zestful', 'zesty']\n",
      "2098 unique words so far.\n",
      "Processing level 4...\n",
      "0 unique new bonus words generated for level 4 so far.\n",
      "1 unique new bonus words generated for level 4 so far.\n",
      "2 unique new bonus words generated for level 4 so far.\n",
      "2 unique new bonus words generated for level 4 so far.\n",
      "3 unique new bonus words generated for level 4 so far.\n",
      "6 unique new bonus words generated for level 4 so far.\n",
      "8 unique new bonus words generated for level 4 so far.\n",
      "8 unique new bonus words generated for level 4 so far.\n",
      "9 unique new bonus words generated for level 4 so far.\n",
      "10 unique new bonus words generated for level 4 so far.\n",
      "15 unique new bonus words generated for level 4 so far.\n",
      "15 unique new bonus words generated for level 4 so far.\n",
      "19 unique new bonus words generated for level 4 so far.\n",
      "21 unique new bonus words generated for level 4 so far.\n",
      "21 unique new bonus words generated for level 4 so far.\n",
      "25 unique new bonus words generated for level 4 so far.\n",
      "29 unique new bonus words generated for level 4 so far.\n",
      "33 unique new bonus words generated for level 4 so far.\n",
      "35 unique new bonus words generated for level 4 so far.\n",
      "37 unique new bonus words generated for level 4 so far.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(bonus_words)\n\u001b[1;32m    115\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate 20 new spelling words of similar length and complexity to the following list.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSpell each correctly.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAvoid plurals.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProvide them in a comma-delimited format. Do not alphabetize, and do not start each with the same letter.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLevel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m words: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(random\u001b[38;5;241m.\u001b[39msample(bonus_words,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m10\u001b[39m))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 116\u001b[0m generated_bonus_words \u001b[38;5;241m=\u001b[39m \u001b[43mget_new_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Filter and select only the new words that are not already in the unique_words_so_far and are close in length to the average\u001b[39;00m\n\u001b[1;32m    119\u001b[0m filtered_new_bonus_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m generated_bonus_words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m unique_words_so_far \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m-\u001b[39m average_bonus_word_length) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m]\n",
      "Cell \u001b[0;32mIn[17], line 23\u001b[0m, in \u001b[0;36mget_new_words\u001b[0;34m(model, prompt, max_tokens)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_new_words\u001b[39m(model, prompt, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     16\u001b[0m     data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m     22\u001b[0m     }\n\u001b[0;32m---> 23\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     25\u001b[0m     new_words \u001b[38;5;241m=\u001b[39m [word\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m response_text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mstrip()]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.4/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Define the file paths\n",
    "input_file_path = \"src/data/word_lists_with_ollama_3.json\"\n",
    "output_file_path = \"src/data/word_lists_with_ollama_4.json\"\n",
    "\n",
    "# Define Ollama API parameters\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def get_new_words(model, prompt, max_tokens=100):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.2\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    response_text = response.json()[\"response\"]\n",
    "    new_words = [word.strip() for word in response_text.split(\",\") if word.strip()]\n",
    "    new_words = [\"\".join(char for char in word if char.isalpha() or char == \"'\") for word in new_words]\n",
    "    new_words = new_words[1:]\n",
    "    new_words = [word.lower() for word in new_words]\n",
    "    new_words = [word for word in new_words if len(word) >= 3]\n",
    "    if len(new_words) >= 5:\n",
    "        # print(f\"Returning {min(50, len(new_words))} words\")\n",
    "        return list(set(new_words[:50]))  # Return exactly 10 words\n",
    "    else:\n",
    "        # print(f\"Warning: Less than 5 words generated for prompt: '{prompt}'\")\n",
    "        return list(set(new_words[:50]))\n",
    "\n",
    "# Read the original JSON file\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Load the data into a dictionary\n",
    "word_lists = json.loads(data)\n",
    "\n",
    "# get all of the unique words in all of the levels and bonus levels\n",
    "all_words = []\n",
    "for level in word_lists[\"levels\"]:\n",
    "    all_words.extend(word_lists[\"levels\"][level][\"words\"])\n",
    "    all_words.extend(word_lists[\"levels\"][level][\"bonus\"])\n",
    "\n",
    "all_words = list(set(all_words))\n",
    "\n",
    "# Model name\n",
    "model_name = \"openhermes\"\n",
    "\n",
    "ollama_words = {\n",
    "    \"levels\": {}\n",
    "}  # this will mimic the dictionary structure of the word_lists\n",
    "# create unique_words_so_far as a set\n",
    "unique_words_so_far = set()\n",
    "\n",
    "new_words_by_level = {}  # Temporary storage for new words by level\n",
    "new_bonus_words_by_level = {}  # Temporary storage for new bonus words by level\n",
    "\n",
    "# Iterate through each level\n",
    "for level in word_lists[\"levels\"]:\n",
    "    print(f\"Processing level {level}...\")\n",
    "    # Get words and bonus words from the original lists\n",
    "    words = list(set(word_lists[\"levels\"][level][\"words\"]))\n",
    "    bonus_words = list(set(word_lists[\"levels\"][level][\"bonus\"]))\n",
    "\n",
    "    # Initialize the level's word and bonus word lists in ollama_words\n",
    "    ollama_words[\"levels\"][level] = {\n",
    "        \"words\": words.copy(),\n",
    "        \"bonus\": bonus_words.copy()\n",
    "    }\n",
    "\n",
    "    # update existing words and bonus words to the unique words so far\n",
    "    unique_words_so_far.update(words)\n",
    "    unique_words_so_far.update(bonus_words)\n",
    "    unique_words_so_far = set(unique_words_so_far)  # Ensure uniqueness\n",
    "\n",
    "    # Get the average word length for the level\n",
    "    average_word_length = sum(len(word) for word in words) / len(words)\n",
    "    average_bonus_word_length = sum(len(word) for word in bonus_words) / len(bonus_words)\n",
    "\n",
    "    # Temporary storage for new unique words per level\n",
    "    new_unique_words = []\n",
    "    new_unique_bonus_words = []\n",
    "\n",
    "    # Generate new words for the main word list\n",
    "    if len(words) < 400:\n",
    "        while len(new_unique_words) < 10:\n",
    "            random.shuffle(words)\n",
    "            prompt = f\"Generate 20 new spelling words of similar length and complexity to the following list.\\nSpell each correctly.\\nAvoid plurals.\\nProvide them in a comma-delimited format. Do not alphabetize, and do not start each with the same letter.\\nLevel {level} words: {', '.join(random.sample(words, 10))}\"\n",
    "            generated_words = get_new_words(model_name, prompt)\n",
    "            \n",
    "            # Filter and select only the new words that are not already in the unique_words_so_far and are close in length to the average\n",
    "            filtered_new_words = [word for word in generated_words if word not in unique_words_so_far and abs(len(word) - average_word_length) <= 2]\n",
    "            \n",
    "            new_unique_words.extend(filtered_new_words)\n",
    "            new_unique_words = list(set(new_unique_words))  # Ensure uniqueness\n",
    "            \n",
    "            print(f\"{len(new_unique_words)} unique new words generated for level {level} so far.\")\n",
    "\n",
    "        if new_unique_words:\n",
    "            print(f\"Adding {len(new_unique_words)} new words to level {level}: {new_unique_words}\")\n",
    "            ollama_words[\"levels\"][level][\"words\"].extend(new_unique_words)\n",
    "            unique_words_so_far.update(new_unique_words)  # Update only after confirming uniqueness\n",
    "            new_words_by_level[level] = new_unique_words\n",
    "\n",
    "    # Generate new words for the bonus word list\n",
    "    if len(bonus_words) < 400:\n",
    "        while len(new_unique_bonus_words) < 80:\n",
    "            random.shuffle(bonus_words)\n",
    "            prompt = f\"Generate 20 new spelling words of similar length and complexity to the following list.\\nSpell each correctly.\\nAvoid plurals.\\nProvide them in a comma-delimited format. Do not alphabetize, and do not start each with the same letter.\\nLevel {level} words: {', '.join(random.sample(bonus_words, 10))}\"\n",
    "            generated_bonus_words = get_new_words(model_name, prompt)\n",
    "            \n",
    "            # Filter and select only the new words that are not already in the unique_words_so_far and are close in length to the average\n",
    "            filtered_new_bonus_words = [word for word in generated_bonus_words if word not in unique_words_so_far and abs(len(word) - average_bonus_word_length) <= 2]\n",
    "            \n",
    "            new_unique_bonus_words.extend(filtered_new_bonus_words)\n",
    "            new_unique_bonus_words = list(set(new_unique_bonus_words))  # Ensure uniqueness\n",
    "            \n",
    "            print(f\"{len(new_unique_bonus_words)} unique new bonus words generated for level {level} so far.\")\n",
    "\n",
    "        if new_unique_bonus_words:\n",
    "            print(f\"Adding {len(new_unique_bonus_words)} new bonus words to level {level}: {new_unique_bonus_words}\")\n",
    "            ollama_words[\"levels\"][level][\"bonus\"].extend(new_unique_bonus_words)\n",
    "            unique_words_so_far.update(new_unique_bonus_words)  # Update only after confirming uniqueness\n",
    "            new_bonus_words_by_level[level] = new_unique_bonus_words\n",
    "\n",
    "    unique_words_so_far = set(unique_words_so_far)  # Ensure uniqueness\n",
    "    print(f\"{len(unique_words_so_far)} unique words so far.\")\n",
    "\n",
    "# Print the new words by level\n",
    "for level in new_words_by_level:\n",
    "    if len(new_words_by_level[level]):\n",
    "        print(f\"Level {level} new words: {', '.join(new_words_by_level[level])}\")\n",
    "    if len(new_bonus_words_by_level[level]):\n",
    "        print(f\"Level {level} new bonus words: {', '.join(new_bonus_words_by_level[level])}\")\n",
    "\n",
    "# Save the updated word lists to a new JSON file\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    json.dump(ollama_words, output_file, indent=2)\n",
    "\n",
    "print(f\"Updated word lists with new words have been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Level  Words  Unique  Bonus  Unique Bonus \n",
      "     1    400     400    326           326 \n",
      "     2    400     400    334           334 \n",
      "     3    401     401    325           325 \n",
      "     4    400     400    327           327 \n",
      "     5    407     407    328           328 \n",
      "     6    405     405    329           329 \n",
      "     7    409     409    328           328 \n",
      "     8    401     401    328           328 \n",
      "     9    400     400    328           328 \n",
      "    10    403     403    324           324 \n",
      "    11    401     401    325           325 \n",
      "    12    652     652    324           324 \n",
      "    13    504     504    324           324 \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def word_table(word_list):\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Level\", \"Words\", \"Unique\", \"Bonus\", \"Unique Bonus\"]\n",
    "    table.border = False\n",
    "    table.align = \"r\"\n",
    "    \n",
    "    for level in word_list[\"levels\"]:  # Use word_list here\n",
    "        words = word_list[\"levels\"][level][\"words\"]  # Use word_list here\n",
    "        unique_words = set(words)\n",
    "        bonus = word_list[\"levels\"][level][\"bonus\"]  # Use word_list here\n",
    "        unique_bonus = set(bonus)\n",
    "        table.add_row([level, len(words), len(unique_words), len(bonus), len(unique_bonus)])\n",
    "\n",
    "    print(table)\n",
    "\n",
    "input_file_path = \"src/data/word_lists_with_ollama_3.json\"\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    data = file.read()\n",
    "ollama_word_lists = json.loads(data)\n",
    "word_table(ollama_word_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated word lists have been saved to src/data/word_lists_with_ollama_4.json\n"
     ]
    }
   ],
   "source": [
    "input_file_path = \"src/data/word_lists_with_ollama_3.json\"\n",
    "output_file_path = \"src/data/word_lists_with_ollama_4.json\"\n",
    "\n",
    "# Read the original JSON file\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Load the data into a dictionary\n",
    "word_lists = json.loads(data)\n",
    "\n",
    "output_data = {\n",
    "    \"levels\": {}\n",
    "}\n",
    "\n",
    "# Process each level to keep only the first 400 words and 200 bonus words\n",
    "for level in word_lists[\"levels\"]:\n",
    "    words = word_lists[\"levels\"][level][\"words\"]\n",
    "    bonus_words = word_lists[\"levels\"][level][\"bonus\"]\n",
    "    \n",
    "    # Limit the words to the first 400 and bonus words to the first 200\n",
    "    words = words[:400]\n",
    "    bonus_words = bonus_words[:200]\n",
    "    \n",
    "    output_data[\"levels\"][level] = {\n",
    "        \"words\": words,\n",
    "        \"bonus\": bonus_words\n",
    "    }\n",
    "\n",
    "# Save the updated word lists to a new JSON file\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    json.dump(output_data, output_file, indent=4)\n",
    "\n",
    "print(f\"Updated word lists have been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Level  Words  Unique  Bonus  Unique Bonus \n",
      "     1    400     400    200           200 \n",
      "     2    400     400    200           200 \n",
      "     3    400     400    200           200 \n",
      "     4    400     400    200           200 \n",
      "     5    400     400    200           200 \n",
      "     6    400     400    200           200 \n",
      "     7    400     400    200           200 \n",
      "     8    400     400    200           200 \n",
      "     9    400     400    200           200 \n",
      "    10    400     400    200           200 \n",
      "    11    400     400    200           200 \n",
      "    12    400     400    200           200 \n",
      "    13    400     400    200           200 \n"
     ]
    }
   ],
   "source": [
    "# print the table for the new dataset\n",
    "with open(output_file_path, \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "word_lists = json.loads(data)\n",
    "\n",
    "word_table(word_lists)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
