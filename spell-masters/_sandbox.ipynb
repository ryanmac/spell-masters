{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "this_list = [\"pay\",\"cup\",\"ship\",\"bat\",\"these\",\"show\",\"rut\",\"down\",\"ten\",\"into\",\"then\",\"dog\",\"bat\",\"with\",\"my\",\"dig\",\"note\",\"cut\",\"mom\",\"brick\"]\n",
    "print(len(this_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/homebrew/lib/python3.11/site-packages (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Level  Words  Unique  Bonus  Unique Bonus \n",
      "     1    250     245    168           168 \n",
      "     2    250     250    168           168 \n",
      "     3    250     250    168           168 \n",
      "     4    250     250    168           168 \n",
      "     5    250     250    168           168 \n",
      "     6    250     250    168           168 \n",
      "     7    250     249    168           168 \n",
      "     8    250     250    168           168 \n",
      "     9    250     250    168           168 \n",
      "    10    250     250    168           168 \n",
      "    11    250     250    168           168 \n",
      "    12    504     502    168           168 \n",
      "    13    350     348    168           168 \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"src/data/word_lists.json\"\n",
    "\n",
    "# Read the JSON file\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Load the data into a dictionary\n",
    "word_lists = json.loads(data)\n",
    "\n",
    "# Print the number of words in each level\n",
    "def word_table(word_list):\n",
    "    from prettytable import PrettyTable\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Level\", \"Words\", \"Unique\", \"Bonus\", \"Unique Bonus\"]\n",
    "    table.border = False\n",
    "    table.align = \"r\"\n",
    "    for level in word_lists[\"levels\"]:\n",
    "        words = word_lists[\"levels\"][level][\"words\"]\n",
    "        unique_words = set(words)\n",
    "        bonus = word_lists[\"levels\"][level][\"bonus\"]\n",
    "        unique_bonus = set(bonus)\n",
    "        table.add_row([level, len(words), len(unique_words), len(bonus), len(unique_bonus)])\n",
    "\n",
    "    print(table)\n",
    "\n",
    "word_table(word_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique word lists have been saved to src/data/word_lists_unique.json\n",
      " Level  Words  Unique  Bonus  Unique Bonus \n",
      "     1    245     245    168           168 \n",
      "     2    250     250    168           168 \n",
      "     3    250     250    168           168 \n",
      "     4    250     250    168           168 \n",
      "     5    249     249    168           168 \n",
      "     6    250     250    168           168 \n",
      "     7    249     249    168           168 \n",
      "     8    250     250    168           168 \n",
      "     9    250     250    168           168 \n",
      "    10    250     250    168           168 \n",
      "    11    250     250    168           168 \n",
      "    12    502     502    168           168 \n",
      "    13    348     348    168           168 \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the file paths\n",
    "input_file_path = \"src/data/word_lists.json\"\n",
    "output_file_path = \"src/data/word_lists_unique.json\"\n",
    "\n",
    "# Read the original JSON file\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Load the data into a dictionary\n",
    "word_lists = json.loads(data)\n",
    "\n",
    "# Create a new dictionary to store the unique words\n",
    "unique_word_lists = {\n",
    "    \"levels\": {},\n",
    "    \"metadata\": word_lists[\"metadata\"]  # Copy the metadata as it is\n",
    "}\n",
    "\n",
    "# Process each level to keep only unique words, ensuring they are lowercase and stripped\n",
    "for level in word_lists[\"levels\"]:\n",
    "    words = word_lists[\"levels\"][level][\"words\"]\n",
    "    bonus_words = word_lists[\"levels\"][level][\"bonus\"]\n",
    "    \n",
    "    # Create sets with words that are lowercased and stripped of leading/trailing whitespace\n",
    "    unique_words = list(set(word.strip().lower() for word in words))\n",
    "    unique_bonus_words = list(set(bonus_word.strip().lower() for bonus_word in bonus_words))\n",
    "    \n",
    "    unique_word_lists[\"levels\"][level] = {\n",
    "        \"words\": unique_words,\n",
    "        \"bonus\": unique_bonus_words\n",
    "    }\n",
    "\n",
    "# Save the unique words dictionary to a new JSON file\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    json.dump(unique_word_lists, output_file, indent=4)\n",
    "\n",
    "print(f\"Unique word lists have been saved to {output_file_path}\")\n",
    "\n",
    "# Reuse the word_table function to verify\n",
    "def word_table(word_list):\n",
    "    from prettytable import PrettyTable\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Level\", \"Words\", \"Unique\", \"Bonus\", \"Unique Bonus\"]\n",
    "    table.border = False\n",
    "    table.align = \"r\"\n",
    "    for level in word_lists[\"levels\"]:\n",
    "        words = word_list[\"levels\"][level][\"words\"]\n",
    "        unique_words = set(words)\n",
    "        bonus = word_list[\"levels\"][level][\"bonus\"]\n",
    "        unique_bonus = set(bonus)\n",
    "        table.add_row([level, len(words), len(unique_words), len(bonus), len(unique_bonus)])\n",
    "\n",
    "    print(table)\n",
    "\n",
    "word_table(unique_word_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.3.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting httpx<0.28.0,>=0.27.0 (from ollama)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting anyio (from httpx<0.28.0,>=0.27.0->ollama)\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.6.2)\n",
      "Collecting httpcore==1.* (from httpx<0.28.0,>=0.27.0->ollama)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in /opt/homebrew/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.4)\n",
      "Collecting sniffio (from httpx<0.28.0,>=0.27.0->ollama)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading ollama-0.3.2-py3-none-any.whl (10 kB)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sniffio, h11, httpcore, anyio, httpx, ollama\n",
      "Successfully installed anyio-4.4.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 ollama-0.3.2 sniffio-1.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"openhermes\",\n",
    "    \"prompt\": \"Tell me a joke\",\n",
    "    \"stream\": False,\n",
    "    \"max_tokens\": 50\n",
    "}\n",
    "\n",
    "import requests\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "response_text = response.json()[\"response\"]\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing level 1...\n",
      "3 unique new bonus words generated for level 1 so far.\n",
      "3 unique new bonus words generated for level 1 so far.\n",
      "5 unique new bonus words generated for level 1 so far.\n",
      "8 unique new bonus words generated for level 1 so far.\n",
      "15 unique new bonus words generated for level 1 so far.\n",
      "15 unique new bonus words generated for level 1 so far.\n",
      "19 unique new bonus words generated for level 1 so far.\n",
      "19 unique new bonus words generated for level 1 so far.\n",
      "26 unique new bonus words generated for level 1 so far.\n",
      "40 unique new bonus words generated for level 1 so far.\n",
      "41 unique new bonus words generated for level 1 so far.\n",
      "43 unique new bonus words generated for level 1 so far.\n",
      "44 unique new bonus words generated for level 1 so far.\n",
      "46 unique new bonus words generated for level 1 so far.\n",
      "49 unique new bonus words generated for level 1 so far.\n",
      "49 unique new bonus words generated for level 1 so far.\n",
      "50 unique new bonus words generated for level 1 so far.\n",
      "52 unique new bonus words generated for level 1 so far.\n",
      "52 unique new bonus words generated for level 1 so far.\n",
      "55 unique new bonus words generated for level 1 so far.\n",
      "59 unique new bonus words generated for level 1 so far.\n",
      "61 unique new bonus words generated for level 1 so far.\n",
      "61 unique new bonus words generated for level 1 so far.\n",
      "64 unique new bonus words generated for level 1 so far.\n",
      "66 unique new bonus words generated for level 1 so far.\n",
      "66 unique new bonus words generated for level 1 so far.\n",
      "68 unique new bonus words generated for level 1 so far.\n",
      "68 unique new bonus words generated for level 1 so far.\n",
      "70 unique new bonus words generated for level 1 so far.\n",
      "70 unique new bonus words generated for level 1 so far.\n",
      "70 unique new bonus words generated for level 1 so far.\n",
      "71 unique new bonus words generated for level 1 so far.\n",
      "71 unique new bonus words generated for level 1 so far.\n",
      "72 unique new bonus words generated for level 1 so far.\n",
      "72 unique new bonus words generated for level 1 so far.\n",
      "74 unique new bonus words generated for level 1 so far.\n",
      "76 unique new bonus words generated for level 1 so far.\n",
      "80 unique new bonus words generated for level 1 so far.\n",
      "Adding 80 new bonus words to level 1: ['tiger', 'scape', 'gusto', 'taint', 'plum', 'pithy', 'gulf', 'wince', 'brine', 'cream', 'pork', 'stall', 'claw', 'scour', 'pelt', 'hatch', 'raze', 'gait', 'weld', 'fuss', 'hunch', 'wide', 'drake', 'silt', 'pane', 'frost', 'fret', 'cull', 'gnome', 'mirth', 'kale', 'stir', 'zeal', 'gnarl', 'lane', 'pate', 'bare', 'rash', 'stake', 'bolt', 'stoop', 'claim', 'gaze', 'tinge', 'grit', 'spare', 'hark', 'quake', 'dais', 'cane', 'vase', 'blade', 'clot', 'flask', 'kelp', 'cove', 'zone', 'shrug', 'fork', 'slush', 'plush', 'rind', 'dour', 'muck', 'gear', 'ditch', 'floss', 'brawn', 'quail', 'snipe', 'rake', 'grime', 'mossy', 'knave', 'dash', 'welt', 'vane', 'snoop', 'swift', 'swath']\n",
      "746 unique words so far.\n",
      "Processing level 2...\n",
      "3 unique new bonus words generated for level 2 so far.\n",
      "5 unique new bonus words generated for level 2 so far.\n",
      "5 unique new bonus words generated for level 2 so far.\n",
      "7 unique new bonus words generated for level 2 so far.\n",
      "8 unique new bonus words generated for level 2 so far.\n",
      "11 unique new bonus words generated for level 2 so far.\n",
      "19 unique new bonus words generated for level 2 so far.\n",
      "20 unique new bonus words generated for level 2 so far.\n",
      "25 unique new bonus words generated for level 2 so far.\n",
      "26 unique new bonus words generated for level 2 so far.\n",
      "28 unique new bonus words generated for level 2 so far.\n",
      "29 unique new bonus words generated for level 2 so far.\n",
      "31 unique new bonus words generated for level 2 so far.\n",
      "34 unique new bonus words generated for level 2 so far.\n",
      "35 unique new bonus words generated for level 2 so far.\n",
      "36 unique new bonus words generated for level 2 so far.\n",
      "38 unique new bonus words generated for level 2 so far.\n",
      "40 unique new bonus words generated for level 2 so far.\n",
      "40 unique new bonus words generated for level 2 so far.\n",
      "41 unique new bonus words generated for level 2 so far.\n",
      "42 unique new bonus words generated for level 2 so far.\n",
      "43 unique new bonus words generated for level 2 so far.\n",
      "45 unique new bonus words generated for level 2 so far.\n",
      "48 unique new bonus words generated for level 2 so far.\n",
      "49 unique new bonus words generated for level 2 so far.\n",
      "50 unique new bonus words generated for level 2 so far.\n",
      "50 unique new bonus words generated for level 2 so far.\n",
      "50 unique new bonus words generated for level 2 so far.\n",
      "50 unique new bonus words generated for level 2 so far.\n",
      "54 unique new bonus words generated for level 2 so far.\n",
      "54 unique new bonus words generated for level 2 so far.\n",
      "60 unique new bonus words generated for level 2 so far.\n",
      "60 unique new bonus words generated for level 2 so far.\n",
      "65 unique new bonus words generated for level 2 so far.\n",
      "67 unique new bonus words generated for level 2 so far.\n",
      "67 unique new bonus words generated for level 2 so far.\n",
      "68 unique new bonus words generated for level 2 so far.\n",
      "70 unique new bonus words generated for level 2 so far.\n",
      "71 unique new bonus words generated for level 2 so far.\n",
      "71 unique new bonus words generated for level 2 so far.\n",
      "72 unique new bonus words generated for level 2 so far.\n",
      "74 unique new bonus words generated for level 2 so far.\n",
      "82 unique new bonus words generated for level 2 so far.\n",
      "Adding 82 new bonus words to level 2: ['rafter', 'turtle', 'bungle', 'quota', 'pummel', 'clover', 'gales', 'nestle', 'frizz', 'jolt', 'fussed', 'bazaar', 'troll', 'drowsy', 'plies', 'rant', 'cram', 'nectar', 'oasis', 'fiddle', 'velvet', 'creaky', 'cajole', 'crusty', 'idiocy', 'coifs', 'tango', 'giddy', 'lurch', 'scurry', 'pallor', 'zigzag', 'outrun', 'whiten', 'stale', 'mishap', 'oar', 'tease', 'dabbed', 'flora', 'heed', 'fizzy', 'flaxen', 'laden', 'wanes', 'pulse', 'oval', 'sally', 'glower', 'coven', 'rancid', 'chisel', 'guffaw', 'chomp', 'sneer', 'bevy', 'wax', 'plunk', 'rasp', 'dwelt', 'groan', 'jostle', 'twinge', 'lenses', 'squint', 'wager', 'napped', 'pebble', 'tide', 'frenzy', 'grove', 'humid', 'muzzle', 'squawk', 'twitch', 'thaw', 'spritz', 'dulcet', 'wring', 'whimsy', 'charge', 'spicy']\n",
      "1423 unique words so far.\n",
      "Processing level 3...\n",
      "2 unique new bonus words generated for level 3 so far.\n",
      "5 unique new bonus words generated for level 3 so far.\n",
      "6 unique new bonus words generated for level 3 so far.\n",
      "7 unique new bonus words generated for level 3 so far.\n",
      "8 unique new bonus words generated for level 3 so far.\n",
      "9 unique new bonus words generated for level 3 so far.\n",
      "14 unique new bonus words generated for level 3 so far.\n",
      "14 unique new bonus words generated for level 3 so far.\n",
      "14 unique new bonus words generated for level 3 so far.\n",
      "14 unique new bonus words generated for level 3 so far.\n",
      "16 unique new bonus words generated for level 3 so far.\n",
      "18 unique new bonus words generated for level 3 so far.\n",
      "20 unique new bonus words generated for level 3 so far.\n",
      "24 unique new bonus words generated for level 3 so far.\n",
      "25 unique new bonus words generated for level 3 so far.\n",
      "28 unique new bonus words generated for level 3 so far.\n",
      "30 unique new bonus words generated for level 3 so far.\n",
      "31 unique new bonus words generated for level 3 so far.\n",
      "31 unique new bonus words generated for level 3 so far.\n",
      "32 unique new bonus words generated for level 3 so far.\n",
      "32 unique new bonus words generated for level 3 so far.\n",
      "32 unique new bonus words generated for level 3 so far.\n",
      "35 unique new bonus words generated for level 3 so far.\n",
      "35 unique new bonus words generated for level 3 so far.\n",
      "36 unique new bonus words generated for level 3 so far.\n",
      "40 unique new bonus words generated for level 3 so far.\n",
      "41 unique new bonus words generated for level 3 so far.\n",
      "42 unique new bonus words generated for level 3 so far.\n",
      "42 unique new bonus words generated for level 3 so far.\n",
      "43 unique new bonus words generated for level 3 so far.\n",
      "43 unique new bonus words generated for level 3 so far.\n",
      "43 unique new bonus words generated for level 3 so far.\n",
      "44 unique new bonus words generated for level 3 so far.\n",
      "45 unique new bonus words generated for level 3 so far.\n",
      "48 unique new bonus words generated for level 3 so far.\n",
      "49 unique new bonus words generated for level 3 so far.\n",
      "53 unique new bonus words generated for level 3 so far.\n",
      "57 unique new bonus words generated for level 3 so far.\n",
      "58 unique new bonus words generated for level 3 so far.\n",
      "58 unique new bonus words generated for level 3 so far.\n",
      "58 unique new bonus words generated for level 3 so far.\n",
      "58 unique new bonus words generated for level 3 so far.\n",
      "58 unique new bonus words generated for level 3 so far.\n",
      "64 unique new bonus words generated for level 3 so far.\n",
      "65 unique new bonus words generated for level 3 so far.\n",
      "66 unique new bonus words generated for level 3 so far.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Define the file paths\n",
    "input_file_path = \"src/data/word_lists_with_ollama_3.json\"\n",
    "output_file_path = \"src/data/word_lists_with_ollama_4.json\"\n",
    "\n",
    "# Define Ollama API parameters\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def get_new_words(model, prompt, max_tokens=100):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.2\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    response_text = response.json()[\"response\"]\n",
    "    new_words = [word.strip() for word in response_text.split(\",\") if word.strip()]\n",
    "    new_words = [\"\".join(char for char in word if char.isalpha() or char == \"'\") for word in new_words]\n",
    "    new_words = new_words[1:]\n",
    "    new_words = [word.lower() for word in new_words]\n",
    "    new_words = [word for word in new_words if len(word) >= 3]\n",
    "    if len(new_words) >= 5:\n",
    "        # print(f\"Returning {min(50, len(new_words))} words\")\n",
    "        return list(set(new_words[:50]))  # Return exactly 10 words\n",
    "    else:\n",
    "        # print(f\"Warning: Less than 5 words generated for prompt: '{prompt}'\")\n",
    "        return list(set(new_words[:50]))\n",
    "\n",
    "# Read the original JSON file\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Load the data into a dictionary\n",
    "word_lists = json.loads(data)\n",
    "\n",
    "# get all of the unique words in all of the levels and bonus levels\n",
    "all_words = []\n",
    "for level in word_lists[\"levels\"]:\n",
    "    all_words.extend(word_lists[\"levels\"][level][\"words\"])\n",
    "    all_words.extend(word_lists[\"levels\"][level][\"bonus\"])\n",
    "\n",
    "all_words = list(set(all_words))\n",
    "\n",
    "# Model name\n",
    "model_name = \"openhermes\"\n",
    "\n",
    "ollama_words = {\n",
    "    \"levels\": {}\n",
    "}  # this will mimic the dictionary structure of the word_lists\n",
    "# create unique_words_so_far as a set\n",
    "unique_words_so_far = set()\n",
    "\n",
    "new_words_by_level = {}  # Temporary storage for new words by level\n",
    "new_bonus_words_by_level = {}  # Temporary storage for new bonus words by level\n",
    "\n",
    "# Iterate through each level\n",
    "for level in word_lists[\"levels\"]:\n",
    "    print(f\"Processing level {level}...\")\n",
    "    # Get words and bonus words from the original lists\n",
    "    words = list(set(word_lists[\"levels\"][level][\"words\"]))\n",
    "    bonus_words = list(set(word_lists[\"levels\"][level][\"bonus\"]))\n",
    "\n",
    "    # Initialize the level's word and bonus word lists in ollama_words\n",
    "    ollama_words[\"levels\"][level] = {\n",
    "        \"words\": words.copy(),\n",
    "        \"bonus\": bonus_words.copy()\n",
    "    }\n",
    "\n",
    "    # update existing words and bonus words to the unique words so far\n",
    "    unique_words_so_far.update(words)\n",
    "    unique_words_so_far.update(bonus_words)\n",
    "    unique_words_so_far = set(unique_words_so_far)  # Ensure uniqueness\n",
    "\n",
    "    # Get the average word length for the level\n",
    "    average_word_length = sum(len(word) for word in words) / len(words)\n",
    "    average_bonus_word_length = sum(len(word) for word in bonus_words) / len(bonus_words)\n",
    "\n",
    "    # Temporary storage for new unique words per level\n",
    "    new_unique_words = []\n",
    "    new_unique_bonus_words = []\n",
    "\n",
    "    # Generate new words for the main word list\n",
    "    if len(words) < 400:\n",
    "        while len(new_unique_words) < 10:\n",
    "            random.shuffle(words)\n",
    "            prompt = f\"Generate 20 new spelling words of similar length and complexity to the following list.\\nSpell each correctly.\\nAvoid plurals.\\nProvide them in a comma-delimited format. Do not alphabetize, and do not start each with the same letter.\\nLevel {level} words: {', '.join(random.sample(words, 10))}\"\n",
    "            generated_words = get_new_words(model_name, prompt)\n",
    "            \n",
    "            # Filter and select only the new words that are not already in the unique_words_so_far and are close in length to the average\n",
    "            filtered_new_words = [word for word in generated_words if word not in unique_words_so_far and abs(len(word) - average_word_length) <= 2]\n",
    "            \n",
    "            new_unique_words.extend(filtered_new_words)\n",
    "            new_unique_words = list(set(new_unique_words))  # Ensure uniqueness\n",
    "            \n",
    "            print(f\"{len(new_unique_words)} unique new words generated for level {level} so far.\")\n",
    "\n",
    "        if new_unique_words:\n",
    "            print(f\"Adding {len(new_unique_words)} new words to level {level}: {new_unique_words}\")\n",
    "            ollama_words[\"levels\"][level][\"words\"].extend(new_unique_words)\n",
    "            unique_words_so_far.update(new_unique_words)  # Update only after confirming uniqueness\n",
    "            new_words_by_level[level] = new_unique_words\n",
    "\n",
    "    # Generate new words for the bonus word list\n",
    "    if len(bonus_words) < 400:\n",
    "        while len(new_unique_bonus_words) < 80:\n",
    "            random.shuffle(bonus_words)\n",
    "            prompt = f\"Generate 20 new spelling words of similar length and complexity to the following list.\\nSpell each correctly.\\nAvoid plurals.\\nProvide them in a comma-delimited format. Do not alphabetize, and do not start each with the same letter.\\nLevel {level} words: {', '.join(random.sample(bonus_words, 10))}\"\n",
    "            generated_bonus_words = get_new_words(model_name, prompt)\n",
    "            \n",
    "            # Filter and select only the new words that are not already in the unique_words_so_far and are close in length to the average\n",
    "            filtered_new_bonus_words = [word for word in generated_bonus_words if word not in unique_words_so_far and abs(len(word) - average_bonus_word_length) <= 2]\n",
    "            \n",
    "            new_unique_bonus_words.extend(filtered_new_bonus_words)\n",
    "            new_unique_bonus_words = list(set(new_unique_bonus_words))  # Ensure uniqueness\n",
    "            \n",
    "            print(f\"{len(new_unique_bonus_words)} unique new bonus words generated for level {level} so far.\")\n",
    "\n",
    "        if new_unique_bonus_words:\n",
    "            print(f\"Adding {len(new_unique_bonus_words)} new bonus words to level {level}: {new_unique_bonus_words}\")\n",
    "            ollama_words[\"levels\"][level][\"bonus\"].extend(new_unique_bonus_words)\n",
    "            unique_words_so_far.update(new_unique_bonus_words)  # Update only after confirming uniqueness\n",
    "            new_bonus_words_by_level[level] = new_unique_bonus_words\n",
    "\n",
    "    unique_words_so_far = set(unique_words_so_far)  # Ensure uniqueness\n",
    "    print(f\"{len(unique_words_so_far)} unique words so far.\")\n",
    "\n",
    "# Print the new words by level\n",
    "for level in new_words_by_level:\n",
    "    if len(new_words_by_level[level]):\n",
    "        print(f\"Level {level} new words: {', '.join(new_words_by_level[level])}\")\n",
    "    if len(new_bonus_words_by_level[level]):\n",
    "        print(f\"Level {level} new bonus words: {', '.join(new_bonus_words_by_level[level])}\")\n",
    "\n",
    "# Save the updated word lists to a new JSON file\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    json.dump(ollama_words, output_file, indent=2)\n",
    "\n",
    "print(f\"Updated word lists with new words have been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Level  Words  Unique  Bonus  Unique Bonus \n",
      "     1    400     400    326           326 \n",
      "     2    400     400    334           334 \n",
      "     3    401     401    325           325 \n",
      "     4    400     400    327           327 \n",
      "     5    407     407    328           328 \n",
      "     6    405     405    329           329 \n",
      "     7    409     409    328           328 \n",
      "     8    401     401    328           328 \n",
      "     9    400     400    328           328 \n",
      "    10    403     403    324           324 \n",
      "    11    401     401    325           325 \n",
      "    12    652     652    324           324 \n",
      "    13    504     504    324           324 \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def word_table(word_list):\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Level\", \"Words\", \"Unique\", \"Bonus\", \"Unique Bonus\"]\n",
    "    table.border = False\n",
    "    table.align = \"r\"\n",
    "    \n",
    "    for level in word_list[\"levels\"]:  # Use word_list here\n",
    "        words = word_list[\"levels\"][level][\"words\"]  # Use word_list here\n",
    "        unique_words = set(words)\n",
    "        bonus = word_list[\"levels\"][level][\"bonus\"]  # Use word_list here\n",
    "        unique_bonus = set(bonus)\n",
    "        table.add_row([level, len(words), len(unique_words), len(bonus), len(unique_bonus)])\n",
    "\n",
    "    print(table)\n",
    "\n",
    "input_file_path = \"src/data/word_lists_with_ollama_3.json\"\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    data = file.read()\n",
    "ollama_word_lists = json.loads(data)\n",
    "word_table(ollama_word_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"src/data/word_lists_with_ollama_3.json\"\n",
    "output_file_path = \"src/data/word_lists_with_ollama_4.json\"\n",
    "\n",
    "# Read the original JSON file\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Load the data into a dictionary\n",
    "word_lists = json.loads(data)\n",
    "\n",
    "output_data = {\n",
    "    \"levels\": {}\n",
    "}\n",
    "\n",
    "# Process each level to keep only the first 400 words and 200 bonus words\n",
    "for level in word_lists[\"levels\"]:\n",
    "    words = word_lists[\"levels\"][level][\"words\"]\n",
    "    bonus_words = word_lists[\"levels\"][level][\"bonus\"]\n",
    "    \n",
    "    # Limit the words to the first 400 and bonus words to the first 200\n",
    "    words = words[:400]\n",
    "    bonus_words = bonus_words[:200]\n",
    "    \n",
    "    output_data[\"levels\"][level] = {\n",
    "        \"words\": words,\n",
    "        \"bonus\": bonus_words\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
